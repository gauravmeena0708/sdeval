# Synthetic Data Evaluator (sdeval)

A comprehensive toolkit for evaluating synthetic tabular datasets against real training data. Compute statistical fidelity, coverage, privacy metrics, and generate comparison visualizations.

[![Tests](https://github.com/gauravmeena0708/sdeval/workflows/tests/badge.svg)](https://github.com/gauravmeena0708/sdeval/actions)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)

## üéØ Features

- **12+ Evaluation Metrics** across 4 categories:
  - Statistical Fidelity (5 metrics)
  - Coverage & Diversity (4 metrics)
  - Privacy Analysis (3 metrics)
  - Constraint Satisfaction (new!)
- **Automatic column type detection** (numerical/categorical)
- **Visualization tools** for distribution comparisons
- **Constraint satisfaction rate** - measure how well synthetic data preserves categorical constraints
- **No configuration required** - just point to your CSV files
- **Comprehensive test suite** (80 tests)
- **Production-ready** with full test coverage

## üì¶ Installation

```bash
# Clone the repository
git clone https://github.com/gauravmeena0708/sdeval.git
cd sdeval

# Install dependencies
pip install -e .

# Or install with development dependencies
pip install -e ".[dev]"
```

### Requirements

- Python 3.9+
- pandas
- numpy
- scipy
- scikit-learn
- matplotlib
- seaborn

## üöÄ Quick Start

### Example: Evaluating Adult Dataset

```python
from pathlib import Path
from sdeval.data_loader import load_csv, detect_column_types
from sdeval.metrics.statistical import compute_statistical_metrics
from sdeval.metrics.coverage import compute_coverage_metrics
from sdeval.metrics.privacy import compute_privacy_metrics
from sdeval.visualization import create_distribution_plots

# Load real and synthetic data
real_df = load_csv(Path("datasets/adult/train.csv"))
synthetic_df = load_csv(Path("datasets/adult/test.csv"))

# Auto-detect column types
col_types = detect_column_types(real_df)
print(f"Numerical columns: {col_types['numerical_columns']}")
print(f"Categorical columns: {col_types['categorical_columns']}")

# Compute statistical fidelity metrics
stats = compute_statistical_metrics(
    real_df, synthetic_df,
    col_types['numerical_columns'],
    col_types['categorical_columns']
)
print(f"Alpha Precision: {stats['alpha_precision']:.3f}")
print(f"Beta Recall: {stats['beta_recall']:.3f}")
print(f"Mean Absolute Difference: {stats['mean_abs_mean_diff']:.3f}")
print(f"Std Absolute Difference: {stats['mean_abs_std_diff']:.3f}")
print(f"Wasserstein Distance: {stats['avg_wasserstein']:.3f}")

# Compute coverage metrics
coverage = compute_coverage_metrics(
    real_df, synthetic_df,
    col_types['categorical_columns']
)
print(f"Uniqueness Ratio: {coverage['uniqueness_ratio']:.3f}")
print(f"Rare Category Retention: {coverage['rare_category_retention']:.3f}")
print(f"Missing Category Ratio: {coverage['missing_category_ratio']:.3f}")
print(f"Missingness Delta: {coverage['missingness_delta']:.3f}")

# Compute privacy metrics (use samples for large datasets)
privacy = compute_privacy_metrics(
    real_df.sample(n=1000, random_state=42),
    synthetic_df.sample(n=500, random_state=42),
    col_types['numerical_columns']
)
print(f"DCR Rate: {privacy['dcr_rate']:.3f}")
print(f"NNDR Mean: {privacy['nndr_mean']:.3f}")
print(f"Mean k-NN Distance: {privacy['mean_knn_distance']:.3f}")

# Generate visualization
create_distribution_plots(
    real_df, synthetic_df,
    col_types['numerical_columns'][:3],  # First 3 numerical columns
    col_types['categorical_columns'][:3],  # First 3 categorical columns
    Path("outputs/distributions.png")
)
print("Visualization saved to outputs/distributions.png")
```

### Complete Evaluation Pipeline

```python
import json
from pathlib import Path
from sdeval.data_loader import load_csv, detect_column_types
from sdeval.metrics.statistical import compute_statistical_metrics
from sdeval.metrics.coverage import compute_coverage_metrics
from sdeval.metrics.privacy import compute_privacy_metrics
from sdeval.visualization import create_distribution_plots

# Load data
real_df = load_csv(Path("datasets/adult/train.csv"))
synthetic_df = load_csv(Path("datasets/adult/test.csv"))
col_types = detect_column_types(real_df)

# Compute all metrics
results = {
    'metadata': {
        'real_data_rows': len(real_df),
        'synthetic_data_rows': len(synthetic_df),
        'numerical_columns': col_types['numerical_columns'],
        'categorical_columns': col_types['categorical_columns']
    },
    'statistical': compute_statistical_metrics(
        real_df, synthetic_df,
        col_types['numerical_columns'],
        col_types['categorical_columns']
    ),
    'coverage': compute_coverage_metrics(
        real_df, synthetic_df,
        col_types['categorical_columns']
    ),
    'privacy': compute_privacy_metrics(
        real_df.sample(n=min(1000, len(real_df)), random_state=42),
        synthetic_df.sample(n=min(500, len(synthetic_df)), random_state=42),
        col_types['numerical_columns']
    )
}

# Save results
output_dir = Path("outputs")
output_dir.mkdir(exist_ok=True)

with open(output_dir / "evaluation_results.json", 'w') as f:
    json.dump(results, f, indent=2)

# Generate visualization
create_distribution_plots(
    real_df, synthetic_df,
    col_types['numerical_columns'],
    col_types['categorical_columns'],
    output_dir / "distributions.png"
)

print("Evaluation complete!")
print(f"Results saved to {output_dir}")
```

### Expected Output

```json
{
  "metadata": {
    "real_data_rows": 32561,
    "synthetic_data_rows": 16281,
    "numerical_columns": ["age", "fnlwgt", "education.num", "capital.gain", "capital.loss", "hours.per.week"],
    "categorical_columns": ["workclass", "education", "marital.status", "occupation", "relationship", "race", "sex", "native.country", "income"]
  },
  "statistical": {
    "alpha_precision": 0.956,
    "beta_recall": 0.943,
    "mean_abs_mean_diff": 245.67,
    "mean_abs_std_diff": 123.45,
    "avg_wasserstein": 0.034
  },
  "coverage": {
    "uniqueness_ratio": 0.987,
    "rare_category_retention": 0.823,
    "missing_category_ratio": 0.045,
    "missingness_delta": 0.001
  },
  "privacy": {
    "dcr_rate": 0.012,
    "nndr_mean": 0.678,
    "mean_knn_distance": 2.341
  }
}
```

## üìä Metrics Explained

### Statistical Fidelity Metrics

**1. Alpha Precision** (0.0 - 1.0, higher is better)
- Fraction of synthetic categorical values that exist in the real data
- Measures if synthetic data introduces invalid/unseen categories
- Example: If synthetic data only contains categories from real data, alpha = 1.0

**2. Beta Recall** (0.0 - 1.0, higher is better)
- Fraction of real categorical values covered by synthetic data
- Measures how well synthetic data represents real data diversity
- Example: If synthetic covers all real categories, beta = 1.0

**3. Mean Absolute Difference** (lower is better)
- Average absolute difference in column means between real and synthetic
- Measures if numerical distributions have similar central tendencies
- Example: If real age mean = 42 and synthetic = 41, contributes |42-41| = 1.0

**4. Std Absolute Difference** (lower is better)
- Average absolute difference in column standard deviations
- Measures if numerical distributions have similar spread
- Example: Checks if synthetic data has similar variability to real

**5. Wasserstein Distance** (lower is better)
- Earth Mover's Distance between real and synthetic distributions
- Measures overall distributional similarity for numerical columns
- Lower values indicate distributions are more similar

### Coverage & Diversity Metrics

**1. Uniqueness Ratio** (0.0 - 1.0, higher is better)
- Fraction of unique rows in synthetic data
- Measures if synthetic data avoids duplicates
- Example: 100 rows with 95 unique ‚Üí ratio = 0.95

**2. Rare Category Retention** (0.0 - 1.0, higher is better)
- Fraction of rare real categories (< 5% frequency) present in synthetic
- Measures if synthetic preserves minority groups
- Example: Real has 10 rare categories, synthetic has 8 ‚Üí retention = 0.80

**3. Missing Category Ratio** (0.0 - 1.0, lower is better)
- Fraction of real categories absent in synthetic data
- Measures coverage gaps
- Example: Real has 100 categories, synthetic missing 5 ‚Üí ratio = 0.05

**4. Missingness Delta** (lower is better)
- Absolute difference in null/missing value rates
- Measures if synthetic preserves missingness patterns
- Example: Real has 2% nulls, synthetic has 3% ‚Üí delta = 0.01

### Privacy Metrics

**1. DCR - Distance to Closest Record** (0.0 - 1.0, lower is better)
- Fraction of synthetic records suspiciously close to real records
- Uses threshold of 1e-8 in standardized feature space
- Lower values indicate better privacy (less memorization)
- Example: If 5 out of 100 synthetic records are too close ‚Üí DCR = 0.05

**2. NNDR - Nearest Neighbor Distance Ratio** (higher is better)
- Ratio of distance to nearest vs second-nearest real neighbor
- Lower ratios indicate synthetic is suspiciously close to one specific record
- Higher values indicate synthetic doesn't memorize specific records
- Example: If synthetic is equally far from top 2 neighbors ‚Üí ratio ‚âà 1.0

**3. Mean k-NN Distance** (higher is better)
- Average distance from synthetic to nearest real record
- Larger distances indicate better privacy
- Measured in standardized feature space
- Example: Higher values mean synthetic is distinct from real data


### Constraint Satisfaction Metrics (NEW!)

The constraint satisfaction module allows you to measure how well synthetic data preserves categorical constraints from the real data.

**Use Case**: When you generate synthetic data with specific constraints (e.g., "generate samples where education=Bachelors" or "workclass=State-gov,education=Bachelors"), you want to verify that the constraint is satisfied at the expected rate.

#### Basic Usage

```python
from sdeval.metrics.constraints import (
    compute_constraint_satisfaction_rate,
    compute_constraint_support
)

# Single constraint - measure satisfaction rate in one dataset
real_rate = compute_constraint_satisfaction_rate(real_df, "education=Bachelors")
print(f"Real data: {real_rate:.2%} samples have education=Bachelors")

# Compare real vs synthetic
metrics = compute_constraint_support(real_df, synthetic_df, "education=Bachelors")
print(f"Real satisfaction rate: {metrics['real_satisfaction_rate']:.2%}")
print(f"Synthetic satisfaction rate: {metrics['synthetic_satisfaction_rate']:.2%}")
print(f"Absolute difference: {metrics['satisfaction_rate_diff']:.4f}")
```

#### Multiple Constraints

Constraints can be combined with comma separation (AND logic):

```python
# Multiple constraints - both must be satisfied
constraint = "workclass=State-gov,education=Bachelors"
metrics = compute_constraint_support(real_df, synthetic_df, constraint)

# This measures: what % of samples have BOTH workclass=State-gov AND education=Bachelors
```

#### Complete Example

```python
from pathlib import Path
from sdeval.data_loader import load_csv
from sdeval.metrics.constraints import compute_constraint_support

# Load data
real_df = load_csv(Path("datasets/adult/train.csv"))
synthetic_df = load_csv(Path("datasets/adult/test.csv"))

# Example 1: Single constraint
constraint1 = "education=11th"
result1 = compute_constraint_support(real_df, synthetic_df, constraint1)
print(f"Constraint: {constraint1}")
print(f"  Real: {result1['real_satisfaction_rate']:.2%}")
print(f"  Synthetic: {result1['synthetic_satisfaction_rate']:.2%}")
print(f"  Difference: {result1['satisfaction_rate_diff']:.4f}")

# Example 2: Multiple constraints
constraint2 = "workclass=State-gov,education=Bachelors"
result2 = compute_constraint_support(real_df, synthetic_df, constraint2)
print(f"\nConstraint: {constraint2}")
print(f"  Real: {result2['real_satisfaction_rate']:.2%}")
print(f"  Synthetic: {result2['synthetic_satisfaction_rate']:.2%}")
print(f"  Difference: {result2['satisfaction_rate_diff']:.4f}")

# Example 3: Batch evaluation across multiple values
workclass_values = ['State-gov', 'Private', 'Federal-gov', 'Local-gov']
for wc in workclass_values:
    result = compute_constraint_support(real_df, synthetic_df, f"workclass={wc}")
    print(f"{wc:20s} Real: {result['real_satisfaction_rate']:6.2%}  "
          f"Synth: {result['synthetic_satisfaction_rate']:6.2%}  "
          f"Diff: {result['satisfaction_rate_diff']:6.4f}")
```

#### Output Metrics

`compute_constraint_support()` returns a dictionary with:
- **real_satisfaction_rate**: Fraction of real samples satisfying the constraint (0.0 - 1.0)
- **synthetic_satisfaction_rate**: Fraction of synthetic samples satisfying the constraint (0.0 - 1.0)
- **satisfaction_rate_diff**: Absolute difference between the two rates

**Interpretation**:
- **Low difference** (< 0.05): Synthetic data preserves the constraint distribution well
- **Medium difference** (0.05 - 0.15): Synthetic data has noticeable deviation
- **High difference** (> 0.15): Synthetic data does not preserve the constraint well

#### Notes

- Constraint values are automatically trimmed (whitespace removed) for robustness
- Empty constraint string returns 1.0 (all samples satisfy vacuous truth)
- Multiple constraints use AND logic (all must be satisfied)
- Best used for categorical columns; numerical constraints not supported yet

See `example_constraint_evaluation.py` for a complete working example.


## üé® Visualization

The toolkit generates comparison plots showing real vs synthetic distributions:

- **Numerical columns**: Overlaid density plots (KDE)
- **Categorical columns**: Side-by-side bar charts
- **Multi-column layout**: Automatic grid arrangement

Example visualization:

![Distribution Comparison](docs/example_distributions.png)

## üß™ Running Tests

```bash
# Run all tests
pytest tests/

# Run specific test file
pytest tests/test_statistical_metrics.py -v

# Run with coverage report
pytest tests/ --cov=sdeval --cov-report=html

# Run only integration tests
pytest tests/test_integration.py -v
```

### Test Structure

```
tests/
‚îú‚îÄ‚îÄ test_data_loader.py          # Data loading & column detection (7 tests)
‚îú‚îÄ‚îÄ test_statistical_metrics.py  # Statistical fidelity (16 tests)
‚îú‚îÄ‚îÄ test_coverage_metrics.py     # Coverage & diversity (16 tests)
‚îú‚îÄ‚îÄ test_privacy_metrics.py      # Privacy analysis (13 tests)
‚îú‚îÄ‚îÄ test_visualization.py        # Visualization generation (6 tests)
‚îî‚îÄ‚îÄ test_integration.py          # End-to-end workflows (3 tests)
```

**Total: 61 tests** covering all functionality

## üìÅ Project Structure

```
sdeval/
‚îú‚îÄ‚îÄ sdeval/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py          # CSV loading & column detection
‚îÇ   ‚îú‚îÄ‚îÄ visualization.py        # Distribution plots
‚îÇ   ‚îî‚îÄ‚îÄ metrics/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py         # Metric registry
‚îÇ       ‚îú‚îÄ‚îÄ statistical.py      # Statistical fidelity metrics
‚îÇ       ‚îú‚îÄ‚îÄ coverage.py         # Coverage & diversity metrics
‚îÇ       ‚îî‚îÄ‚îÄ privacy.py          # Privacy analysis metrics
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_*.py               # Comprehensive test suite
‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îî‚îÄ‚îÄ adult/
‚îÇ       ‚îú‚îÄ‚îÄ train.csv           # Example real data
‚îÇ       ‚îî‚îÄ‚îÄ test.csv            # Example synthetic data
‚îú‚îÄ‚îÄ pyproject.toml              # Project dependencies
‚îú‚îÄ‚îÄ README.md                   # This file
‚îî‚îÄ‚îÄ plan.md                     # Implementation checklist
```

## üî¨ Example: Adult Dataset

The Adult dataset (Census Income) is used throughout the examples:

**Dataset Details:**
- **Source**: UCI Machine Learning Repository
- **Real data**: 32,561 rows (train.csv)
- **Synthetic data**: 16,281 rows (test.csv)
- **Numerical columns** (6): age, fnlwgt, education.num, capital.gain, capital.loss, hours.per.week
- **Categorical columns** (9): workclass, education, marital.status, occupation, relationship, race, sex, native.country, income

**Typical Results:**
- Alpha Precision: ~0.95 (95% of synthetic categories exist in real)
- Beta Recall: ~0.94 (94% of real categories covered)
- Uniqueness Ratio: ~0.99 (99% unique rows)
- DCR Rate: ~0.01 (1% records too close)

## üõ†Ô∏è Development

### Setting Up Development Environment

```bash
# Clone repository
git clone https://github.com/gauravmeena0708/sdeval.git
cd sdeval

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install in development mode
pip install -e ".[dev]"

# Run tests
pytest tests/
```

### Adding New Metrics

Metrics follow a simple pattern:

```python
from typing import Dict, List
import pandas as pd
from sdeval.metrics import register_metric, MetricContext

def compute_my_metric(real_df: pd.DataFrame,
                      synthetic_df: pd.DataFrame,
                      numerical_columns: List[str]) -> float:
    """
    Compute your custom metric.

    Args:
        real_df: Real dataset
        synthetic_df: Synthetic dataset
        numerical_columns: List of numerical column names

    Returns:
        Metric value
    """
    # Your implementation here
    return 0.0

# For registry integration (optional)
@register_metric("my_metric")
def compute_my_metric_registry(ctx: MetricContext) -> Dict[str, float]:
    return {
        'my_metric': compute_my_metric(
            ctx.real_df,
            ctx.synthetic_df,
            ctx.numerical_columns
        )
    }
```

## üìù Citation

If you use this toolkit in your research, please cite:

```bibtex
@software{sdeval2025,
  title={Synthetic Data Evaluator: A Comprehensive Toolkit for Evaluating Synthetic Tabular Data},
  author={Your Name},
  year={2025},
  url={https://github.com/gauravmeena0708/sdeval}
}
```

## ü§ù Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Add tests for new functionality
4. Ensure all tests pass (`pytest tests/`)
5. Commit your changes (`git commit -m 'Add amazing feature'`)
6. Push to the branch (`git push origin feature/amazing-feature`)
7. Open a Pull Request

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

## üôè Acknowledgments

- UCI Machine Learning Repository for the Adult dataset
- scikit-learn for machine learning utilities
- scipy for statistical functions
- matplotlib/seaborn for visualization

## üìß Contact

For questions or issues, please open an issue on GitHub or contact [your-email@example.com]

---

**Built with ‚ù§Ô∏è using Test-Driven Development**
